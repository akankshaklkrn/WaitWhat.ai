# WaitWhat.ai - Demo Therapist ğŸ¯

**Analyze demo videos for clarity issues and provide actionable feedback with humor**

> Problem: Good ideas lose because explanations break under pressure; feedback is vague.  
> Solution: Upload a demo video â†’ detect "confusion-risk conditions" â†’ show exact timestamps + evidence â†’ suggest minimal fixes â†’ humor wrapper (Roast Mode).

## ğŸ¥ MVP Demo Flow

1. **Upload** a demo/pitch video
2. **Analyze** â†’ AI detects clarity issues
3. **View** list of flagged moments with timestamps
4. **Click** jumps video to exact issue
5. **Roast slider** changes feedback tone (Kind/Honest/Brutal)

## ğŸ—ï¸ Architecture

```
Frontend (Next.js + Tailwind)
    â†“ Upload video
Backend (FastAPI + Python)
    â†“ Index with TwelveLabs
    â†“ Get transcript + timestamps
    â†“ Analyze with Gemini
    â†“ Detect 6 clarity signals
Frontend (React)
    â†“ Display issues list
    â†“ Video player with seek
    â†“ Roast mode toggle
```

## ğŸ“ Project Structure

```
WaitWhat.ai/
â”œâ”€â”€ backend/                    # Person A + Person B
â”‚   â”œâ”€â”€ llm_tools.py           # âœ… Person B: Gemini integration
â”‚   â”œâ”€â”€ signal_helpers.py      # âœ… Person B: Local computations
â”‚   â”œâ”€â”€ config.py              # âœ… Configuration
â”‚   â”œâ”€â”€ requirements.txt       # âœ… Dependencies
â”‚   â”‚
â”‚   â”œâ”€â”€ example_integration.py # âœ… Working example
â”‚   â”œâ”€â”€ test_llm_tools.py     # âœ… Test suite
â”‚   â”œâ”€â”€ quickstart_person_a.py # âœ… Template for Person A
â”‚   â”‚
â”‚   â”œâ”€â”€ README.md              # Backend overview
â”‚   â””â”€â”€ README_PERSON_B.md     # Person B detailed docs
â”‚
â”œâ”€â”€ frontend/                   # Person C (Next.js)
â”‚   â””â”€â”€ (to be built)
â”‚
â””â”€â”€ README.md                   # This file
```

## ğŸš€ Quick Start

### Person B (LLM Tools) - âœ… COMPLETE

```bash
cd backend
pip install -r requirements.txt
export GEMINI_API_KEY="your-key"
python test_llm_tools.py
```

**Status:** Ready for integration!  
**Docs:** See `backend/README_PERSON_B.md`

### Person A (Backend)

```bash
cd backend
pip install -r requirements.txt

# Set API keys
export GEMINI_API_KEY="your-gemini-key"
export TWELVE_LABS_API_KEY="your-twelvelabs-key"

# Start with template
python quickstart_person_a.py

# Or run full example
python example_integration.py
```

**TODO:**
- [ ] FastAPI endpoints (`/upload`, `/analyze`)
- [ ] TwelveLabs video indexing
- [ ] Transcript windowing
- [x] Integrate Person B's LLM tools (example provided)

**Template:** `backend/quickstart_person_a.py`

### Person C (Frontend)

```bash
# Coming soon
cd frontend
npm install
npm run dev
```

**TODO:**
- [ ] Next.js + Tailwind setup
- [ ] Video player component
- [ ] Issues list + detail view
- [ ] Roast mode slider
- [ ] Clarity score display

**API Endpoint:** `POST http://localhost:8000/analyze`

## ğŸ¯ The 6 Clarity Signals

| Signal | What It Detects | Implementation |
|--------|----------------|----------------|
| **1. Concept Spike** | Too many buzzwords at once | `llm.extract_terms()` |
| **2. Grounding Gap** | Terms used before defined | `llm.check_term_definition()` |
| **3. Trust-Me-Bro** | Claims without evidence | `llm.classify_claims_evidence()` |
| **4. Visual Mismatch** | Speech â‰  visuals | TwelveLabs + Gemini |
| **5. Structure Order** | Bad pitch flow | `llm.role_tag()` |
| **6. Ramble Ratio** | Filler words, low density | `SignalHelpers.analyze_ramble()` |

## ğŸ“Š Data Flow

```
1. Upload Video
   â””â†’ Backend saves file, returns video_id

2. TwelveLabs Processing
   â”œâ†’ Index video
   â”œâ†’ Extract transcript with timestamps
   â””â†’ Split into 10s windows

3. AI Analysis (Per Window)
   â”œâ†’ Extract technical terms (Gemini)
   â”œâ†’ Detect claims vs evidence (Gemini)
   â”œâ†’ Tag segment role (Gemini)
   â”œâ†’ Check term definitions (Gemini)
   â””â†’ Compute local signals (no API)

4. Risk Scoring
   â”œâ†’ Combine signal severities
   â”œâ†’ Compute risk score
   â””â†’ Flag high-risk segments

5. Issue Generation
   â”œâ†’ Generate catchy label (Gemini)
   â”œâ†’ Suggest fix (Gemini)
   â””â†’ Create 3 roast tones (Gemini)

6. Frontend Display
   â”œâ†’ Show clarity score + tier
   â”œâ†’ List flagged segments
   â”œâ†’ Click to seek video
   â””â†’ Toggle roast mode
```

## ğŸ› ï¸ Tech Stack

| Component | Technology | Why |
|-----------|-----------|-----|
| Frontend | Next.js + Tailwind | Fast UI, modern |
| Backend | FastAPI (Python) | Quick endpoints |
| Video Analysis | TwelveLabs | Indexing + transcripts |
| AI | Google Gemini | LLM analysis |
| Hosting | Vercel + Render | Fast deployment |

## ğŸ“ Example Output

```json
{
  "clarity_score": 63,
  "clarity_tier": "Wait...what are we building?",
  "segments": [
    {
      "segment_id": 1,
      "start_sec": 10,
      "end_sec": 20,
      "label": "Buzzword Overdose",
      "fix": "Define Marengo in one sentence before this segment.",
      "tone": {
        "kind": "This part introduces multiple terms quickly. Add a definition first.",
        "honest": "You dropped 3 acronyms with zero grounding. Define Marengo first.",
        "brutal": "Acronym speedrun detected. Judges met Marengo 0 seconds agoâ€”introduce it first."
      }
    }
  ]
}
```

## ğŸ† Hackathon Strategy

### MLH Prize Targets

**Best Use of Gemini:**
- âœ… Heavy Gemini usage for all LLM tasks
- âœ… Term extraction, claims detection, role tagging
- âœ… Label generation, roast text generation

**Best Use of TwelveLabs:**
- Video indexing with timestamps
- Semantic search for visual context
- Transcript extraction

### Demo Prep

1. **Prepare 2 test videos:**
   - One intentionally bad (lots of flags)
   - One decent (fewer flags)

2. **Pre-load sample video** in UI (avoid live upload delays)

3. **Practice pitch:**
   - Problem: Vague feedback on demos
   - Solution: AI-powered clarity analysis
   - Demo: Show flagged segments + roast mode
   - Wow factor: Time-coded receipts with humor

## ğŸ§ª Testing

### Test Person B's Tools
```bash
cd backend
python test_llm_tools.py
```

### Test Integration Example
```bash
cd backend
python example_integration.py
```

### Test Backend Server
```bash
cd backend
python quickstart_person_a.py
# Visit http://localhost:8000/docs
```

## ğŸ“š Documentation

- **`backend/README.md`** - Backend overview + team coordination
- **`backend/README_PERSON_B.md`** - Detailed LLM tools documentation
- **`backend/example_integration.py`** - Complete working pipeline
- **`backend/quickstart_person_a.py`** - FastAPI template for Person A

## âš™ï¸ Configuration

Create `.env` file in `backend/`:

```bash
# Required
GEMINI_API_KEY=your-gemini-key
TWELVE_LABS_API_KEY=your-twelvelabs-key

# Optional
GEMINI_MODEL=gemini-1.5-flash  # or gemini-1.5-pro
RISK_THRESHOLD=4.0
DEBUG=true
```

Get API keys:
- **Gemini:** https://makersuite.google.com/app/apikey
- **TwelveLabs:** https://dashboard.twelvelabs.io/

## ğŸš¢ Deployment

### Option 1: Local + ngrok (Fastest)
```bash
# Terminal 1: Backend
cd backend
python quickstart_person_a.py

# Terminal 2: ngrok
ngrok http 8000

# Terminal 3: Frontend
cd frontend
npm run dev
```

### Option 2: Production
- **Frontend:** Vercel (auto-deploy from GitHub)
- **Backend:** Render (FastAPI)
- **Storage:** Filesystem or MongoDB Atlas

## âœ… Progress Checklist

### Person B - LLM Tools âœ… COMPLETE
- [x] `llm_tools.py` with all 5 functions
- [x] `signal_helpers.py` for local signals
- [x] Configuration management
- [x] Test suite
- [x] Integration example
- [x] Documentation

### Person A - Backend â³ IN PROGRESS
- [ ] FastAPI setup
- [ ] `/upload` endpoint
- [ ] `/analyze` endpoint
- [ ] TwelveLabs integration
- [ ] Transcript windowing
- [x] Person B tools integration (template provided)

### Person C - Frontend â³ TODO
- [ ] Next.js setup
- [ ] Video player
- [ ] Issues list
- [ ] Roast slider
- [ ] Clarity score display

## ğŸ¤ Team Coordination

**Person A needs from Person B:** âœ… DONE
- All functions ready in `llm_tools.py`
- Helper functions in `signal_helpers.py`
- Integration example provided
- Template with integration shown

**Person C needs from Person A:** â³ WAITING
- `POST /upload` endpoint
- `POST /analyze` endpoint returning JSON
- See `quickstart_person_a.py` for expected response format

**Person B needs from others:** âœ… NOTHING
- Deliverables complete and documented

## ğŸ’¡ Development Tips

### For Speed
1. Use `gemini-1.5-flash` (default) - faster
2. Enable regex fallbacks in term extraction
3. Use local signals where possible
4. Cache analysis results

### For Quality
1. Switch to `gemini-1.5-pro` - better accuracy
2. Disable fallbacks for pure LLM results
3. Add retry logic for API failures
4. Fine-tune signal weights

### For Demo
1. Pre-load test video (avoid upload time)
2. Have backup screenshots if API fails
3. Practice roast mode toggle (it's the wow factor)
4. Show "before/after" of fixing a flagged issue

## ğŸ› Troubleshooting

### Backend won't start
```bash
pip install -r requirements.txt
export GEMINI_API_KEY="your-key"
```

### Tests fail
```bash
# Check API key
echo $GEMINI_API_KEY

# Test internet connection
curl https://generativelanguage.googleapis.com/v1/models

# Run verbose tests
python test_llm_tools.py
```

### Frontend can't reach backend
```bash
# Check CORS in FastAPI
# Check backend is running: curl http://localhost:8000
# Check network: ping localhost
```

## ğŸ“ Getting Help

- **Person B (LLM):** Check `backend/README_PERSON_B.md`
- **Person A (Backend):** See `backend/example_integration.py`
- **Person C (Frontend):** API spec in `backend/quickstart_person_a.py`

## ğŸ‰ Project Status

| Component | Status | Owner |
|-----------|--------|-------|
| LLM Tools | âœ… Complete | Person B |
| Signal Helpers | âœ… Complete | Person B |
| Documentation | âœ… Complete | Person B |
| Backend API | â³ In Progress | Person A |
| TwelveLabs Integration | â³ Todo | Person A |
| Frontend | â³ Todo | Person C |
| Deployment | â³ Todo | Team |

---

**Built for Joke Hack 2026** ğŸ¯

**Team:** Person A (Backend), Person B (LLM), Person C (Frontend)

**Goal:** Help presenters avoid the "Wait...what?" moment

Good luck! ğŸš€
